{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10d5ba8",
   "metadata": {},
   "source": [
    "# get_fdt test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dd12c3",
   "metadata": {},
   "source": [
    "## TO-DO\n",
    "- sort by values or by index\n",
    "- fmt_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38fb7b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard Libs\n",
    "from typing import Union, Optional, Tuple, Literal, Any\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "# Third-Party Libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "# # Local Libs\n",
    "# from jm_datascience import jm_pandas as jm_pd\n",
    "# from jm_datascience import jm_pdaccessor as jm\n",
    "# from jm_utils import jm_richprt as jm_prt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3282540",
   "metadata": {},
   "source": [
    "## Some Series and DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4919c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_work = pd.DataFrame({\n",
    "    'nombre': ['Ana', 'Bob', '', 'Carlos', ' ', 'Diana'],\n",
    "    'apellido': ['A_Ana', 'B_Bob', None, 'C_Carlos', None, 'D_Diana'],\n",
    "    'edad': [25, -1, 30, 999, 28, 22],\n",
    "    'ciudad': ['Madrid', 'N/A', 'Barcelona', 'Valencia', 'unknown', 'Sevilla'],\n",
    "    'salario': [50000, 0, 60000, -999, 55000, 48000]\n",
    "})\n",
    "\n",
    "## Read spreedsheet for tests\n",
    "try:\n",
    "    spreedsheet = r\"C:\\Users\\jm\\Documents\\__Dev\\PortableGit\\__localrepos\\365DS_jm\\3_statistics\\2_13_Practical_Ex_Descriptive_Stats.xlsx\"    # Casa\n",
    "    with open(spreedsheet) as f:\n",
    "        pass\n",
    "except FileNotFoundError:\n",
    "    spreedsheet = r\"D:\\git\\PortableGit\\__localrepos\\365DS_jm\\3_statistics\\2_13_Practical_Ex_Descriptive_Stats.xlsx\"                         # Office\n",
    "\n",
    "df_xls = pd.read_excel(spreedsheet, skiprows=4, usecols='B:J,L:AA', index_col='ID')\n",
    "df = df_xls.copy()\n",
    "\n",
    "lst_str = random.choices([chr(i) for i in range(65, 72)], k=175)\n",
    "# sr_str = jm_pd.to_series(lst_str)                         # <- jm_pd.to_serie_with_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f9cee95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['California', 'Virginia', 'Arizona', 'Oregon', 'Nevada',\n",
       "       'Colorado', 'Utah', nan, 'Kansas', 'Wyoming'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Country\n",
       "Germany      1\n",
       "Mexico       1\n",
       "Denmark      1\n",
       "UK           2\n",
       "Belgium      2\n",
       "Russia       4\n",
       "Canada       7\n",
       "USA         12\n",
       "NaN         72\n",
       "USA        165\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df['State'].unique())\n",
    "df['State'].value_counts(sort=False, ascending=True)\n",
    "df['Country'].value_counts(sort=True, ascending=True, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ad6c3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fmt_value_for_pd(value, width=8, decimals=3, miles=',') -> str:\n",
    "    \"\"\"\n",
    "    Format a value (numeric or string) into a right-aligned string of fixed width.\n",
    "\n",
    "    Converts numeric values to formatted strings with thousands separators and\n",
    "    specified decimal places. Strings are padded to the same width for consistent alignment.\n",
    "\n",
    "    Parameters:\n",
    "        value (int, float, str): The value to be formatted.\n",
    "        width (int): Total width of the output string. Must be a positive integer.\n",
    "        decimals (int): Number of decimal places for numeric values. Must be >= 0.\n",
    "        miles (str or None): Thousands separator. Valid options: ',', '_', or None.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted string with right alignment.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If width <= 0, decimals < 0, or miles is invalid.\n",
    "\n",
    "    Examples:\n",
    "        >>> format_value(123456.789)\n",
    "        '123,456.79'\n",
    "        >>> format_value(\"text\", width=10)\n",
    "        '      text'\n",
    "        >>> format_value(9876, miles=None)\n",
    "        '    9876.00'\n",
    "    \"\"\"\n",
    "    # Parameter Value validation <- vamos a tener que analizar este tema por si es un list , etc,,\n",
    "    #   - En realidad acá tenemos que evaluar algo similar a jm_utils - fmt_values() FUTURE\n",
    "    # if not isinstance(value, (int, float, np.integer, np.floating)) or pd.api.types.is_any_real_numeric_dtype(value)\n",
    "\n",
    "    if not isinstance(width, int) or width <= 0:\n",
    "        raise ValueError(f\"Width must be a positive integer. Not '{width}'\")\n",
    "    \n",
    "    if not isinstance(decimals, int) or decimals < 0:\n",
    "        raise ValueError(f\"Decimals must be a non-negative integer. Not '{decimals}\")\n",
    "    \n",
    "    if miles not in [',', '_', None]:\n",
    "        raise ValueError(f\"Miles must be either ',', '_', or None. Not '{miles}\")\n",
    "    \n",
    "    try:\n",
    "        num = float(value)                                  # Convert to float if possible\n",
    "        if num % 1 == 0:                                    # it its a total integer number\n",
    "            decimals = 0\n",
    "        if miles:\n",
    "            return f\"{num:>{width}{miles}.{decimals}f}\"     # Ancho fijo, x decimales, alineado a la derecha\n",
    "        else:\n",
    "            return f\"{num:>{width}.{decimals}f}\"\n",
    "        \n",
    "    except (ValueError, TypeError):\n",
    "        return str(value).rjust(width)                      # Alinea también strings, para mantener la grilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe510044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_series(\n",
    "    data: Union[pd.Series, np.ndarray, dict, list, set, pd.DataFrame],\n",
    "    index: Optional[pd.Index] = None,\n",
    "    name: Optional[str] = None\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Converts input data into a pandas Series, optionally returning value counts.\n",
    "\n",
    "    This function accepts various data types and converts them into a pandas Series.\n",
    "    If `count=True`, it returns the frequency count of the values in the resulting Series.\n",
    "\n",
    "    Parameters:\n",
    "        data (Union[pd.Series, np.ndarray, dict, list, set, pd.DataFrame]):\n",
    "            The input data to convert. Supported types include:\n",
    "            - pd.Series: returned as-is or counted if `count=True`.\n",
    "            - np.ndarray: flattened and converted to a Series.\n",
    "            - dict: keys become the index, values are used for data.\n",
    "            - list or set: converted directly to a Series.\n",
    "            - pd.DataFrame:\n",
    "                - 1 column: converted directly to a Series.\n",
    "                - 2 columns: first column becomes the index, second becomes the values.\n",
    "\n",
    "        count (bool or int, optional): Whether to return value counts instead of raw data.\n",
    "            If True or 1, returns frequencies of each value. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A pandas Series representing the input data. If `count=True`, returns\n",
    "            the value counts of the data.\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If `data` is not one of the supported types.\n",
    "        ValueError: If `count` is not a boolean or integer 0/1.\n",
    "        ValueError: If DataFrame has more than 2 columns.\n",
    "\n",
    "    Examples:\n",
    "        >>> import pandas as pd\n",
    "        >>> to_serie_with_count([1, 2, 2, 3])\n",
    "        0    1\n",
    "        1    2\n",
    "        2    2\n",
    "        3    3\n",
    "        dtype: int64\n",
    "\n",
    "        >>> to_serie_with_count([1, 2, 2, 3], count=True)\n",
    "        2    2\n",
    "        1    1\n",
    "        3    1\n",
    "        dtype: int64\n",
    "\n",
    "        >>> df = pd.DataFrame({'Category': ['A', 'B', 'A'], 'Value': [10, 20, 30]})\n",
    "        >>> to_serie_with_count(df)\n",
    "        Category\n",
    "        A    10\n",
    "        B    20\n",
    "        A    30\n",
    "        Name: Value, dtype: int64\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate parameters - FUTURE\n",
    "    \n",
    "    if isinstance(data, pd.Series):                 # If data is already a series no conversion needed\n",
    "        sr = data                                  \n",
    "    elif isinstance(data, np.ndarray):              # If data is a NumPy array   \n",
    "        sr = pd.sr(data.flatten())\n",
    "    elif isinstance(data, (dict, list)):\n",
    "        sr = pd.sr(data)\n",
    "    elif isinstance(data, (set)):\n",
    "        sr = pd.sr(tuple(data))\n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "        if data.shape[1] == 1:                      # Also len(data.columns == 1)\n",
    "            sr = data.iloc[:, 0]\n",
    "        elif data.shape[1] == 2:                    # Index: first col, Data: 2nd Col\n",
    "            sr = data.set_index(data.columns[0])[data.columns[1]]\n",
    "        else:\n",
    "            raise ValueError(\"DataFrame must have 1 oer 2 columns. Categories and values for 2 columns cases.\")\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported data type: {type(data)}. \"\n",
    "                    \"Supported types: pd.sr, np.ndarray, pd.DataFrame, dict, list, set, and pd.DataFrame\")\n",
    "\n",
    "    if name:\n",
    "        sr.name = name\n",
    "\n",
    "    if index:\n",
    "        sr.index = index\n",
    "\n",
    "    return sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "389faabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_numeric_series(\n",
    "        data: Union[pd.Series, pd.DataFrame],\n",
    "        positive: Optional[bool] = True\n",
    ") -> Union[None, Exception]:\n",
    "\n",
    "    # Validate data parameter a pandas object\n",
    "    if not isinstance(data, (pd.Series, pd.DataFrame)):     # pd.Series or pd.Datafram\n",
    "        raise TypeError(\n",
    "            f\"Input data must be a pandas Series or DataFrame. Got {type(data)} instead.\"\n",
    "        )\n",
    "              \n",
    "    if positive:\n",
    "        if not all(                                             # Only positve numeric values                 \n",
    "            isinstance(val, (int, float, np.integer, np.floating)) and val > 0 for val in data.values\n",
    "        ):\n",
    "            raise ValueError(f\"All values in 'data' must be positive numeric values.\")\n",
    "        pass\n",
    "    else:                                                       # Just only numeric values\n",
    "        if not all(isinstance(val, (int, float, np.integer, np.floating)) for val in data.values):\n",
    "            raise ValueError(f\"All values in 'data' must be numeric values.\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4ebfc3",
   "metadata": {},
   "source": [
    "## OJO - podemos refactor get_fdt\n",
    "- Podemos impactar el 'order' directamente a la fdt resultante y no a la serie previa\n",
    "- De esta manera podemo hacer que el valor del aside también se ordene pero NOOO.. mejor que si es aside quede al final!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb7068fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fdt0(\n",
    "        data: Union[pd.Series, np.ndarray, dict, list, set, pd.DataFrame],\n",
    "        value_counts: Optional[bool] = False,\n",
    "        dropna: Optional[bool] = True,\n",
    "        na_position: Optional[str] = 'last',\n",
    "        include_pcts: Optional[bool] = True,\n",
    "        include_plain_relatives: Optional[bool] = True,\n",
    "        fmt_values: Optional[bool] = False,\n",
    "        order: Optional[str] = 'desc',\n",
    "        na_aside: Optional[bool] = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a Frequency Distribution Table (FDT) with absolute, relative, and cumulative frequencies.\n",
    "\n",
    "    This function converts various input data types into a structured DataFrame containing:\n",
    "    - Absolute frequencies\n",
    "    - Cumulative frequencies\n",
    "    - Relative frequencies (proportions and percentages)\n",
    "    - Cumulative relative frequencies (percentages)\n",
    "\n",
    "    Parameters:\n",
    "        data (Union[pd.Series, np.ndarray, dict, list, pd.DataFrame]): Input data.\n",
    "            If DataFrame, it will be converted to a Series using `to_series`.\n",
    "        value_counts (bool, optional): Whether to count occurrences if input is raw data.\n",
    "            Assumes data is not pre-counted. Default is False.\n",
    "        dropna (bool, optional): Whether to exclude NaN values when counting frequencies.\n",
    "            Default is True.\n",
    "        na_position (str, optional): Position of NaN values in the output:\n",
    "            - 'first': Place NaN at the top.\n",
    "            - 'last': Place NaN at the bottom (default).\n",
    "            - 'value': Keep NaN in its natural order.\n",
    "            Default is 'last'.\n",
    "        include_pcts (bool, optional): Whether to include percentage columns.\n",
    "            If False, only absolute and cumulative frequencies are returned.\n",
    "            Default is True.\n",
    "        include_plain_relatives (bool, optional): Whether to return relative and cumulative relative values.\n",
    "            If False, only frequency and percentage columns are included.\n",
    "            Default is True.\n",
    "        fmt_values (bool, optional): Whether to format numeric values using `_fmt_value_for_pd`.\n",
    "            Useful for improving readability in reports. Default is False.\n",
    "        order (str, optional): Sort order for the output:\n",
    "            - 'asc': Sort values ascending.\n",
    "            - 'desc': Sort values descending (default).\n",
    "            - 'ix_asc': Sort by index ascending.\n",
    "            - 'ix_desc': Sort by index descending.\n",
    "            - None: No sorting.\n",
    "            Default is 'desc'.\n",
    "        na_aside (bool, optional): Whether to separate NaN values from calculations but keep them in the output.\n",
    "            If True, NaNs are added at the end and not included in cumulative or relative calculations.\n",
    "            Default is True.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the frequency distribution table with the following columns\n",
    "        (depending on parameters):\n",
    "            - Frequency\n",
    "            - Cumulative Frequency\n",
    "            - Relative Frequency\n",
    "            - Cumulative Relative Freq.\n",
    "            - Relative Freq. [%]\n",
    "            - Cumulative Freq. [%]\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `sort` or `na_position` receive invalid values.\n",
    "\n",
    "    Notes:\n",
    "        - This function uses `to_series` to convert input data into a pandas Series.\n",
    "        - If `na_aside=True` and NaNs are present, they are placed separately and not included in relative calculations.\n",
    "        - Useful for exploratory data analysis and generating clean statistical summaries.\n",
    "\n",
    "    Example:\n",
    "        >>> import pandas as pd\n",
    "        >>> data = pd.Series(['A', 'B', 'A', 'C', 'B', 'B', None])\n",
    "        >>> fdt = get_fdt(data, sort='desc', fmt_values=True)\n",
    "        >>> print(fdt)\n",
    "              Frequency  Cumulative Frequency  Relative Freq. [%]  Cumulative Freq. [%]\n",
    "        B           3                   3                42.86                  42.86\n",
    "        A           2                   5                28.57                  71.43\n",
    "        C           1                   6                14.29                  85.71\n",
    "        Nulls       1                   7                14.29                 100.00\n",
    "    \"\"\"\n",
    "    columns = [\n",
    "        'Frequency',\n",
    "        'Cumulative Frequency',\n",
    "        'Relative Frequency',\n",
    "        'Cumulative Relative Freq.',\n",
    "        'Relative Freq. [%]',\n",
    "        'Cumulative Freq. [%]'\n",
    "    ]\n",
    "    # def _calculate_fdt_relatives(series):     # Revisar, no me gusta el flujo actual\n",
    "    \n",
    "    sr = to_series(data)\n",
    "    \n",
    "    if dropna:\n",
    "        sr = sr.dropna()                        # Drop all nulls values of the Series\n",
    "        sr = sr.drop(np.nan, errors='ignore')   # For series with NaNs as a category with their count (errors='ignore': does not fail if it does not exist)\n",
    "\n",
    "    if value_counts:\n",
    "        sr = sr.value_counts(dropna=dropna, sort=False)\n",
    "\n",
    "    # Validate that all the values are positive numbers\n",
    "    if not _validate_numeric_series(sr):\n",
    "        raise ValueError(f\"To get a Frequency Distribution Table all frequencies must by positive numbers\")\n",
    "\n",
    "    # Order de original Series to obtain the fdt in the same order as the original data\n",
    "    match order:\n",
    "        case 'asc':\n",
    "            sr = sr.sort_values()\n",
    "        case 'desc':\n",
    "            sr = sr.sort_values(ascending=False)\n",
    "        case 'ix_asc':\n",
    "            sr = sr.sort_index()\n",
    "        case 'ix_desc':\n",
    "            sr = sr.sort_index(ascending=False)\n",
    "        case None:\n",
    "            pass\n",
    "        case _:\n",
    "            raise ValueError(f\"Valid values for order: 'asc', 'desc', 'ix_asc', 'ix_desc', or None. Got '{order}'\")\n",
    "\n",
    "    # Handle NaN values \n",
    "    try:                            # To manage when there aren't NaNs\n",
    "        nan_value = sr[np.nan]\n",
    "        sr_without_nan = sr.drop(np.nan)\n",
    "    except:\n",
    "        nan_value = 0\n",
    "        sr_without_nan = sr.copy()  # If no NaNs, we keep the original series without changes\n",
    "    finally:\n",
    "        if na_aside:\n",
    "            # Column that will then be concatenated to the end of the DF if the na_aside option is true\n",
    "            nan_row_df = pd.DataFrame(data = [nan_value], columns=[columns[0]], index=['Nulls'])      # Only 'Frequency' column.\n",
    "            if nan_value > 0:\n",
    "                sr = sr_without_nan\n",
    "\n",
    "    match na_position and not na_aside:              # Locate the NaNs values\n",
    "        case 'first':\n",
    "            sr = pd.concat([pd.Series({np.nan: nan_value}), sr_without_nan])\n",
    "        case 'last':\n",
    "            sr = pd.concat([sr_without_nan, pd.Series({np.nan: nan_value})])\n",
    "        case 'value' | None:\n",
    "            pass\n",
    "        case _:\n",
    "            raise ValueError(f\"Valid values for na_position: 'first', 'last', 'value' or None. Got '{na_position}'\")\n",
    "\n",
    "    # Central rutine: create the fdt, including relative and cumulative columns.\n",
    "    fdt = pd.DataFrame(sr)\n",
    "    fdt.columns = [columns[0]]\n",
    "    fdt[columns[1]] = fdt['Frequency'].cumsum()\n",
    "    fdt[columns[2]] = fdt['Frequency'] / fdt['Frequency'].sum()\n",
    "    fdt[columns[3]] = fdt['Relative Frequency'].cumsum()\n",
    "    fdt[columns[4]] = fdt['Relative Frequency'] * 100\n",
    "    fdt[columns[5]] = fdt['Cumulative Relative Freq.'] * 100\n",
    "\n",
    "    if na_aside and not dropna:             # We add nan_columns at the end\n",
    "        fdt = pd.concat([fdt, nan_row_df])\n",
    "\n",
    "    if not include_pcts:                    # Don't return percentage columns\n",
    "        fdt = fdt[columns[0:4]]\n",
    "    \n",
    "    if not include_plain_relatives:         # Don't return relative and plain cumulative\n",
    "        fdt = fdt[[columns[0], columns[4], columns[5]]]\n",
    "\n",
    "    if fmt_values:\n",
    "        fdt = fdt.map(_fmt_value_for_pd)\n",
    "        \n",
    "    return fdt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c4f364",
   "metadata": {},
   "source": [
    "## OJO - podemos refactor get_fdt\n",
    "- Podemos impactar el 'order' directamente a la fdt resultante y no a la serie previa\n",
    "- De esta manera podemo hacer que el valor del aside también se ordene pero NOOO.. mejor que si es aside quede al final!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32acf81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fdt(\n",
    "        data: Union[pd.Series, np.ndarray, dict, list, set, pd.DataFrame],\n",
    "        value_counts: Optional[bool] = False,\n",
    "        dropna: Optional[bool] = True,\n",
    "        na_position: Optional[str] = 'last',\n",
    "        include_pcts: Optional[bool] = True,\n",
    "        include_plain_relatives: Optional[bool] = True,\n",
    "        fmt_values: Optional[bool] = False,\n",
    "        order: Optional[str] = 'desc',\n",
    "        na_aside: Optional[bool] = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a Frequency Distribution Table (FDT) with absolute, relative, and cumulative frequencies.\n",
    "\n",
    "    This function converts various input data types into a structured DataFrame containing:\n",
    "    - Absolute frequencies\n",
    "    - Cumulative frequencies\n",
    "    - Relative frequencies (proportions and percentages)\n",
    "    - Cumulative relative frequencies (percentages)\n",
    "\n",
    "    Parameters:\n",
    "        data (Union[pd.Series, np.ndarray, dict, list, pd.DataFrame]): Input data.\n",
    "            If DataFrame, it will be converted to a Series using `to_series`.\n",
    "        value_counts (bool, optional): Whether to count occurrences if input is raw data.\n",
    "            Assumes data is not pre-counted. Default is False.\n",
    "        dropna (bool, optional): Whether to exclude NaN values when counting frequencies.\n",
    "            Default is True.\n",
    "        na_position (str, optional): Position of NaN values in the output:\n",
    "            - 'first': Place NaN at the top.\n",
    "            - 'last': Place NaN at the bottom (default).\n",
    "            - 'value': Keep NaN in its natural order.\n",
    "            Default is 'last'.\n",
    "        include_pcts (bool, optional): Whether to include percentage columns.\n",
    "            If False, only absolute and cumulative frequencies are returned.\n",
    "            Default is True.\n",
    "        include_plain_relatives (bool, optional): Whether to return relative and cumulative relative values.\n",
    "            If False, only frequency and percentage columns are included.\n",
    "            Default is True.\n",
    "        fmt_values (bool, optional): Whether to format numeric values using `_fmt_value_for_pd`.\n",
    "            Useful for improving readability in reports. Default is False.\n",
    "        order (str, optional): Sort order for the output:\n",
    "            - 'asc': Sort values ascending.\n",
    "            - 'desc': Sort values descending (default).\n",
    "            - 'ix_asc': Sort by index ascending.\n",
    "            - 'ix_desc': Sort by index descending.\n",
    "            - None: No sorting.\n",
    "            Default is 'desc'.\n",
    "        na_aside (bool, optional): Whether to separate NaN values from calculations but keep them in the output.\n",
    "            If True, NaNs are added at the end and not included in cumulative or relative calculations.\n",
    "            Default is True.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the frequency distribution table with the following columns\n",
    "        (depending on parameters):\n",
    "            - Frequency\n",
    "            - Cumulative Frequency\n",
    "            - Relative Frequency\n",
    "            - Cumulative Relative Freq.\n",
    "            - Relative Freq. [%]\n",
    "            - Cumulative Freq. [%]\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `sort` or `na_position` receive invalid values.\n",
    "\n",
    "    Notes:\n",
    "        - This function uses `to_series` to convert input data into a pandas Series.\n",
    "        - If `na_aside=True` and NaNs are present, they are placed separately and not included in relative calculations.\n",
    "        - Useful for exploratory data analysis and generating clean statistical summaries.\n",
    "\n",
    "    Example:\n",
    "        >>> import pandas as pd\n",
    "        >>> data = pd.Series(['A', 'B', 'A', 'C', 'B', 'B', None])\n",
    "        >>> fdt = get_fdt(data, sort='desc', fmt_values=True)\n",
    "        >>> print(fdt)\n",
    "              Frequency  Cumulative Frequency  Relative Freq. [%]  Cumulative Freq. [%]\n",
    "        B           3                   3                42.86                  42.86\n",
    "        A           2                   5                28.57                  71.43\n",
    "        C           1                   6                14.29                  85.71\n",
    "        Nulls       1                   7                14.29                 100.00\n",
    "    \"\"\"\n",
    "    columns = [\n",
    "        'Frequency',\n",
    "        'Cumulative Frequency',\n",
    "        'Relative Frequency',\n",
    "        'Cumulative Relative Freq.',\n",
    "        'Relative Freq. [%]',\n",
    "        'Cumulative Freq. [%]'\n",
    "    ]\n",
    "    # def _calculate_fdt_relatives(series):     # Revisar, no me gusta el flujo actual\n",
    "    \n",
    "    sr = to_series(data)\n",
    "    \n",
    "    if dropna:\n",
    "        sr = sr.dropna()                        # Drop all nulls values of the Series\n",
    "        sr = sr.drop(np.nan, errors='ignore')   # For series with NaNs as a category with their count (errors='ignore': does not fail if it does not exist)\n",
    "\n",
    "    if value_counts:\n",
    "        sr = sr.value_counts(dropna=dropna, sort=False)\n",
    "\n",
    "    # Validate that all the values are positive numbers\n",
    "    _validate_numeric_series(sr)\n",
    "\n",
    "    # Order de original Series to obtain the fdt in the same order as the original data\n",
    "    match order:\n",
    "        case 'asc':\n",
    "            sr = sr.sort_values()\n",
    "        case 'desc':\n",
    "            sr = sr.sort_values(ascending=False)\n",
    "        case 'ix_asc':\n",
    "            sr = sr.sort_index()\n",
    "        case 'ix_desc':\n",
    "            sr = sr.sort_index(ascending=False)\n",
    "        case None:\n",
    "            pass\n",
    "        case _:\n",
    "            raise ValueError(f\"Valid values for order: 'asc', 'desc', 'ix_asc', 'ix_desc', or None. Got '{order}'\")\n",
    "        \n",
    "    # Handle NaNs values. Two cases: 1. na_aside: don't use for calcs and at the end; 2. use for calcs and locate according na_position\n",
    "    #   - Determine the number of nans\n",
    "    if pd.isna(sr.index).any():\n",
    "        n_nans = sr[np.nan]\n",
    "    else:\n",
    "        n_nans = 0\n",
    "\n",
    "    #   - Locale NaNs row in the Series 'sr'\n",
    "    if na_aside:\n",
    "        sr = sr.drop(np.nan, errors='ignore')\n",
    "        # Column that will then be concatenated to the end of the DF - Only 'Frequency' column, no calculated columns\n",
    "        nan_row_df = pd.DataFrame(data = [n_nans], columns=[columns[0]], index=[np.nan])\n",
    "        # nan_row_df = pd.DataFrame(data = [n_nans], columns=[columns[0]], index=['Nulls'])\n",
    "    else:\n",
    "        # As we use NaNs for calculations decide where locate these values\n",
    "        sr_without_nan = sr.drop(np.nan, errors='ignore')\n",
    "        match na_position:             \n",
    "            case 'first':\n",
    "                sr = pd.concat([pd.Series({np.nan: n_nans}), sr_without_nan])\n",
    "            case 'last':\n",
    "                sr = pd.concat([sr_without_nan, pd.Series({np.nan: n_nans})])\n",
    "            case 'value' | None:\n",
    "                pass                # Locates the Nulls row based on the value or index ordering\n",
    "            case _:\n",
    "                raise ValueError(f\"Valid values for na_position: 'first', 'last', 'value' or None. Got '{na_position}'\")\n",
    "\n",
    "    # Central rutine: create the fdt, including relative and cumulative columns.\n",
    "    fdt = pd.DataFrame(sr)\n",
    "    fdt.columns = [columns[0]]\n",
    "    fdt[columns[1]] = fdt['Frequency'].cumsum()\n",
    "    fdt[columns[2]] = fdt['Frequency'] / fdt['Frequency'].sum()\n",
    "    fdt[columns[3]] = fdt['Relative Frequency'].cumsum()\n",
    "    fdt[columns[4]] = fdt['Relative Frequency'] * 100\n",
    "    fdt[columns[5]] = fdt['Cumulative Relative Freq.'] * 100\n",
    "\n",
    "    if na_aside and not dropna:             # We add nan_columns at the end\n",
    "        fdt = pd.concat([fdt, nan_row_df])\n",
    "\n",
    "    if not include_pcts:                    # Don't return percentage columns\n",
    "        fdt = fdt[columns[0:4]]\n",
    "    \n",
    "    if not include_plain_relatives:         # Don't return relative and plain cumulative\n",
    "        fdt = fdt[[columns[0], columns[4], columns[5]]]\n",
    "\n",
    "    if fmt_values:\n",
    "        fdt = fdt.map(_fmt_value_for_pd)\n",
    "        \n",
    "    return fdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09719486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Cumulative Frequency</th>\n",
       "      <th>Relative Frequency</th>\n",
       "      <th>Cumulative Relative Freq.</th>\n",
       "      <th>Relative Freq. [%]</th>\n",
       "      <th>Cumulative Freq. [%]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.404</td>\n",
       "      <td>40.449</td>\n",
       "      <td>40.449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>89</td>\n",
       "      <td>197</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.738</td>\n",
       "      <td>33.333</td>\n",
       "      <td>73.783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>70</td>\n",
       "      <td>267</td>\n",
       "      <td>0.262</td>\n",
       "      <td>1</td>\n",
       "      <td>26.217</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Frequency Cumulative Frequency Relative Frequency  \\\n",
       "Gender                                                     \n",
       "M            108                  108              0.404   \n",
       "NaN           89                  197              0.333   \n",
       "F             70                  267              0.262   \n",
       "\n",
       "       Cumulative Relative Freq. Relative Freq. [%] Cumulative Freq. [%]  \n",
       "Gender                                                                    \n",
       "M                          0.404             40.449               40.449  \n",
       "NaN                        0.738             33.333               73.783  \n",
       "F                              1             26.217                  100  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fdt_s1 = get_fdt(df['State'], value_counts=True, dropna=False, na_position='last', include_pcts=True, include_plain_relatives=True, fmt_values=True, order='desc', na_aside=True)\n",
    "# fdt_s1 = get_fdt(df['Country'], value_counts=True, dropna=False, na_position='last', include_pcts=True, include_plain_relatives=True, fmt_values=True, order='desc', na_aside=True)\n",
    "fdt_g1 = get_fdt(df['Gender'], value_counts=True, dropna=False, na_position='value', include_pcts=True, include_plain_relatives=True, fmt_values=True, order='desc', na_aside=False)\n",
    "fdt_g1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e33dc57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Cumulative Frequency</th>\n",
       "      <th>Relative Frequency</th>\n",
       "      <th>Cumulative Relative Freq.</th>\n",
       "      <th>Relative Freq. [%]</th>\n",
       "      <th>Cumulative Freq. [%]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.657</td>\n",
       "      <td>65.746</td>\n",
       "      <td>65.746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nevada</th>\n",
       "      <td>17</td>\n",
       "      <td>136</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.751</td>\n",
       "      <td>9.392</td>\n",
       "      <td>75.138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colorado</th>\n",
       "      <td>11</td>\n",
       "      <td>147</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.812</td>\n",
       "      <td>6.077</td>\n",
       "      <td>81.215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oregon</th>\n",
       "      <td>11</td>\n",
       "      <td>158</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.873</td>\n",
       "      <td>6.077</td>\n",
       "      <td>87.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arizona</th>\n",
       "      <td>11</td>\n",
       "      <td>169</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.934</td>\n",
       "      <td>6.077</td>\n",
       "      <td>93.370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Utah</th>\n",
       "      <td>6</td>\n",
       "      <td>175</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.967</td>\n",
       "      <td>3.315</td>\n",
       "      <td>96.685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Virginia</th>\n",
       "      <td>4</td>\n",
       "      <td>179</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.989</td>\n",
       "      <td>2.210</td>\n",
       "      <td>98.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kansas</th>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.552</td>\n",
       "      <td>99.448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wyoming</th>\n",
       "      <td>1</td>\n",
       "      <td>181</td>\n",
       "      <td>0.006</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.552</td>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>86</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Frequency Cumulative Frequency Relative Frequency  \\\n",
       "California       119                  119              0.657   \n",
       "Nevada            17                  136              0.094   \n",
       "Colorado          11                  147              0.061   \n",
       "Oregon            11                  158              0.061   \n",
       "Arizona           11                  169              0.061   \n",
       "Utah               6                  175              0.033   \n",
       "Virginia           4                  179              0.022   \n",
       "Kansas             1                  180              0.006   \n",
       "Wyoming            1                  181              0.006   \n",
       "NaN               86                  nan                nan   \n",
       "\n",
       "           Cumulative Relative Freq. Relative Freq. [%] Cumulative Freq. [%]  \n",
       "California                     0.657             65.746               65.746  \n",
       "Nevada                         0.751              9.392               75.138  \n",
       "Colorado                       0.812              6.077               81.215  \n",
       "Oregon                         0.873              6.077               87.293  \n",
       "Arizona                        0.934              6.077               93.370  \n",
       "Utah                           0.967              3.315               96.685  \n",
       "Virginia                       0.989              2.210               98.895  \n",
       "Kansas                         0.994              0.552               99.448  \n",
       "Wyoming                        1.000              0.552              100.000  \n",
       "NaN                              nan                nan                  nan  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdt_s1 = get_fdt(df['State'], value_counts=True, dropna=False, na_position='value', include_pcts=True, include_plain_relatives=True, fmt_values=True, order='desc', na_aside=True)\n",
    "fdt_s1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eca3f17a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Cumulative Frequency</th>\n",
       "      <th>Relative Frequency</th>\n",
       "      <th>Cumulative Relative Freq.</th>\n",
       "      <th>Relative Freq. [%]</th>\n",
       "      <th>Cumulative Freq. [%]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.657</td>\n",
       "      <td>65.746</td>\n",
       "      <td>65.746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nevada</th>\n",
       "      <td>17</td>\n",
       "      <td>136</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.751</td>\n",
       "      <td>9.392</td>\n",
       "      <td>75.138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arizona</th>\n",
       "      <td>11</td>\n",
       "      <td>147</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.812</td>\n",
       "      <td>6.077</td>\n",
       "      <td>81.215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oregon</th>\n",
       "      <td>11</td>\n",
       "      <td>158</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.873</td>\n",
       "      <td>6.077</td>\n",
       "      <td>87.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colorado</th>\n",
       "      <td>11</td>\n",
       "      <td>169</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.934</td>\n",
       "      <td>6.077</td>\n",
       "      <td>93.370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Utah</th>\n",
       "      <td>6</td>\n",
       "      <td>175</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.967</td>\n",
       "      <td>3.315</td>\n",
       "      <td>96.685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Virginia</th>\n",
       "      <td>4</td>\n",
       "      <td>179</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.989</td>\n",
       "      <td>2.210</td>\n",
       "      <td>98.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kansas</th>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.552</td>\n",
       "      <td>99.448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wyoming</th>\n",
       "      <td>1</td>\n",
       "      <td>181</td>\n",
       "      <td>0.006</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.552</td>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Frequency Cumulative Frequency Relative Frequency  \\\n",
       "State                                                          \n",
       "California       119                  119              0.657   \n",
       "Nevada            17                  136              0.094   \n",
       "Arizona           11                  147              0.061   \n",
       "Oregon            11                  158              0.061   \n",
       "Colorado          11                  169              0.061   \n",
       "Utah               6                  175              0.033   \n",
       "Virginia           4                  179              0.022   \n",
       "Kansas             1                  180              0.006   \n",
       "Wyoming            1                  181              0.006   \n",
       "\n",
       "           Cumulative Relative Freq. Relative Freq. [%] Cumulative Freq. [%]  \n",
       "State                                                                         \n",
       "California                     0.657             65.746               65.746  \n",
       "Nevada                         0.751              9.392               75.138  \n",
       "Arizona                        0.812              6.077               81.215  \n",
       "Oregon                         0.873              6.077               87.293  \n",
       "Colorado                       0.934              6.077               93.370  \n",
       "Utah                           0.967              3.315               96.685  \n",
       "Virginia                       0.989              2.210               98.895  \n",
       "Kansas                         0.994              0.552               99.448  \n",
       "Wyoming                        1.000              0.552              100.000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc = df['State'].value_counts(sort=False, dropna=True)\n",
    "fdt_vc = get_fdt(vc, dropna=True, na_position='value', include_pcts=True, include_plain_relatives=True, fmt_values=True, order='desc', na_aside=True)\n",
    "fdt_vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d18b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vc.dropna()\n",
    "# vc.drop(np.nan, errors='ignore')\n",
    "# # nans = vc[np.nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12c7d552",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m fdt_vc1 = get_fdt(\u001b[43mvc\u001b[49m, value_counts=\u001b[38;5;28;01mTrue\u001b[39;00m, dropna=\u001b[38;5;28;01mFalse\u001b[39;00m, na_position=\u001b[33m'\u001b[39m\u001b[33mlast\u001b[39m\u001b[33m'\u001b[39m, include_pcts=\u001b[38;5;28;01mTrue\u001b[39;00m, include_plain_relatives=\u001b[38;5;28;01mTrue\u001b[39;00m, fmt_values=\u001b[38;5;28;01mTrue\u001b[39;00m, order=\u001b[33m'\u001b[39m\u001b[33mdesc\u001b[39m\u001b[33m'\u001b[39m, na_aside=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# fdt_vc1 = get_fdt(vc, na_aside=False)\u001b[39;00m\n\u001b[32m      3\u001b[39m fdt_vc1\n",
      "\u001b[31mNameError\u001b[39m: name 'vc' is not defined"
     ]
    }
   ],
   "source": [
    "fdt_vc1 = get_fdt(vc, value_counts=True, dropna=False, na_position='last', include_pcts=True, include_plain_relatives=True, fmt_values=True, order='desc', na_aside=True)\n",
    "# fdt_vc1 = get_fdt(vc, na_aside=False)\n",
    "fdt_vc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9dd21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fdt_s1 = get_fdt(df['Country'], value_counts=True, sort='asc', dropna=False, na_position='value', fmt_values=True, na_aside=False)\n",
    "# fdt_s1\n",
    "fdt_s2 = get_fdt(df['State'], value_counts=True, dropna=False, na_aside=False, na_position='value', fmt_values=True)\n",
    "fdt_s2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f238ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdt_2 = get_fdt(df['State'], value_counts=True)    \n",
    "cumulative_pcts = fdt_2['Cumulative Freq. [%]']\n",
    "top_3_pct = cumulative_pcts.iloc[min(2, len(cumulative_pcts)-1)]\n",
    "\n",
    "labels = [f\"{fdt_2.iloc[ix, 0]} ({fdt_2.iloc[ix, -2]:.1f} %)\" for ix in range(fdt_2.shape[0])]\n",
    "print(labels)\n",
    "\n",
    "for iloc_ix in range(len(cumulative_pcts)):\n",
    "    print(f\"cumulative_pcts.iloc[{iloc_ix}] = {cumulative_pcts.iloc[iloc_ix]}\")\n",
    "\n",
    "display(len(cumulative_pcts))\n",
    "display(top_3_pct)\n",
    "fdt_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd06e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "822d6309",
   "metadata": {},
   "source": [
    "## Some Typing Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f116700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional, Any, Literal, Sequence, TypeAlias\n",
    "import pandas as pd\n",
    "\n",
    "IndexElement: TypeAlias = Union[str, int, float, 'datetime.datetime']\n",
    "\n",
    "def to_series(\n",
    "    data: Union[pd.Series, np.ndarray, dict, list, set, pd.DataFrame],\n",
    "    index: Optional[Union[pd.Index, Sequence[IndexElement]]] = None,\n",
    "    name: Optional[str] = None\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Converts input data into a pandas Series, optionally returning value counts.\n",
    "    \"\"\"\n",
    "    return pd.Series(data, index=index, name=name)\n",
    "\n",
    "\n",
    "to_series([1, 2, 3], ['a', 'b', 'c'], name='example_series')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc319fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some Typing Tests\n",
    "## Standard Libs\n",
    "from typing import Union, Optional, Any, Literal, Sequence, TypeAlias\n",
    "\n",
    "# Third-Party Libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter  # for pareto chart and ?\n",
    "import seaborn as sns\n",
    "## Claude - Qwen\n",
    "\n",
    "\n",
    "## Custom types for non-included typing annotations - Grok\n",
    "IndexElement: TypeAlias = Union[str, int, float, 'datetime.datetime', pd.Timestamp]\n",
    "\n",
    "\n",
    "def test_typing(\n",
    "        value: Union[int, float, str],\n",
    "        data: Optional[Union[pd.Index, Sequence[IndexElement]]] = None,\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Test function to demonstrate typing with Union.\n",
    "\n",
    "    Parameters:\n",
    "        value (Union[int, float, str]): The input value which can be an int, float, or str.\n",
    "\n",
    "    Returns:\n",
    "        str: A string representation of the input value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"Numeric value: {value}\")\n",
    "    elif isinstance(value, str):\n",
    "        print(f\"String value: {value}\")\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported type: {type(value)}\")\n",
    "    \n",
    "    if data is not None:\n",
    "        if isinstance(data, pd.Index):\n",
    "            print(f\"Data is a pandas Index with {len(data)} elements.\")\n",
    "        elif isinstance(data, (list, tuple, np.ndarray)):\n",
    "            print(f\"Data is a sequence with {len(data)} elements.\")\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported data type: {type(data)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3da61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_typing(42, data=pd.Index(['a', 'b', 'c']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94091d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeAlias, Optional, Union, Sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "IndexElement: TypeAlias = Union[str, int, float, 'datetime.datetime', np.str_, np.int64, np.float64, np.datetime64]\n",
    "IndexLike: TypeAlias = Union[pd.Index, Sequence[IndexElement], NDArray[IndexElement]]\n",
    "\n",
    "def mi_funcion(data, index: Optional[IndexLike] = None) -> None:\n",
    "    if index is not None:\n",
    "        if isinstance(index, np.ndarray) and index.ndim != 1:\n",
    "            raise ValueError(\"El array de NumPy debe ser 1D para usarse como índice\")\n",
    "        index = pd.Index(index) if not isinstance(index, pd.Index) else index\n",
    "        print(\"Índice proporcionado:\", index)\n",
    "    else:\n",
    "        print(\"No se proporcionó índice, usando índice por defecto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76514e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_funcion(data=pd.Series([1, 2, 3]), index=np.array(['a', 'b', 'c']))\n",
    "mi_funcion(data=pd.Series([1, 2, 3]), index=pd.Index(['a', 'b', 'c']))\n",
    "mi_funcion(data=pd.Series([1, 2, 3]))  # Sin índice"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
