{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10d5ba8",
   "metadata": {},
   "source": [
    "# get_fdt test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dd12c3",
   "metadata": {},
   "source": [
    "## TO-DO\n",
    "- sort by values or by index\n",
    "- fmt_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38fb7b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard Libs\n",
    "from typing import Union, Optional, Tuple, Literal, Any\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "# Third-Party Libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "# # Local Libs\n",
    "# from jm_datascience import jm_pandas as jm_pd\n",
    "# from jm_datascience import jm_pdaccessor as jm\n",
    "# from jm_utils import jm_richprt as jm_prt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3282540",
   "metadata": {},
   "source": [
    "## Some Series and DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4919c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_work = pd.DataFrame({\n",
    "    'nombre': ['Ana', 'Bob', '', 'Carlos', ' ', 'Diana'],\n",
    "    'apellido': ['A_Ana', 'B_Bob', None, 'C_Carlos', None, 'D_Diana'],\n",
    "    'edad': [25, -1, 30, 999, 28, 22],\n",
    "    'ciudad': ['Madrid', 'N/A', 'Barcelona', 'Valencia', 'unknown', 'Sevilla'],\n",
    "    'salario': [50000, 0, 60000, -999, 55000, 48000]\n",
    "})\n",
    "\n",
    "## Read spreedsheet for tests\n",
    "try:\n",
    "    spreedsheet = r\"C:\\Users\\jm\\Documents\\__Dev\\PortableGit\\__localrepos\\365DS_jm\\3_statistics\\2_13_Practical_Ex_Descriptive_Stats.xlsx\"    # Casa\n",
    "    with open(spreedsheet) as f:\n",
    "        pass\n",
    "except FileNotFoundError:\n",
    "    spreedsheet = r\"D:\\git\\PortableGit\\__localrepos\\365DS_jm\\3_statistics\\2_13_Practical_Ex_Descriptive_Stats.xlsx\"                         # Office\n",
    "\n",
    "df_xls = pd.read_excel(spreedsheet, skiprows=4, usecols='B:J,L:AA', index_col='ID')\n",
    "df = df_xls.copy()\n",
    "\n",
    "lst_str = random.choices([chr(i) for i in range(65, 72)], k=175)\n",
    "# sr_str = jm_pd.to_series(lst_str)                         # <- jm_pd.to_serie_with_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f9cee95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['California', 'Virginia', 'Arizona', 'Oregon', 'Nevada',\n",
       "       'Colorado', 'Utah', nan, 'Kansas', 'Wyoming'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Country\n",
       "Germany      1\n",
       "Mexico       1\n",
       "Denmark      1\n",
       "UK           2\n",
       "Belgium      2\n",
       "Russia       4\n",
       "Canada       7\n",
       "USA         12\n",
       "NaN         72\n",
       "USA        165\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df['State'].unique())\n",
    "df['State'].value_counts(sort=False, ascending=True)\n",
    "df['Country'].value_counts(sort=True, ascending=True, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ad6c3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fmt_value_for_pd(value, width=8, decimals=3, miles=',') -> str:\n",
    "    \"\"\"\n",
    "    Format a value (numeric or string) into a right-aligned string of fixed width.\n",
    "\n",
    "    Converts numeric values to formatted strings with thousands separators and\n",
    "    specified decimal places. Strings are padded to the same width for consistent alignment.\n",
    "\n",
    "    Parameters:\n",
    "        value (int, float, str): The value to be formatted.\n",
    "        width (int): Total width of the output string. Must be a positive integer.\n",
    "        decimals (int): Number of decimal places for numeric values. Must be >= 0.\n",
    "        miles (str or None): Thousands separator. Valid options: ',', '_', or None.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted string with right alignment.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If width <= 0, decimals < 0, or miles is invalid.\n",
    "\n",
    "    Examples:\n",
    "        >>> format_value(123456.789)\n",
    "        '123,456.79'\n",
    "        >>> format_value(\"text\", width=10)\n",
    "        '      text'\n",
    "        >>> format_value(9876, miles=None)\n",
    "        '    9876.00'\n",
    "    \"\"\"\n",
    "    # Parameter Value validation <- vamos a tener que analizar este tema por si es un list , etc,,\n",
    "    #   - En realidad acá tenemos que evaluar algo similar a jm_utils - fmt_values() FUTURE\n",
    "    # if not isinstance(value, (int, float, np.integer, np.floating)) or pd.api.types.is_any_real_numeric_dtype(value)\n",
    "\n",
    "    if not isinstance(width, int) or width <= 0:\n",
    "        raise ValueError(f\"Width must be a positive integer. Not '{width}'\")\n",
    "    \n",
    "    if not isinstance(decimals, int) or decimals < 0:\n",
    "        raise ValueError(f\"Decimals must be a non-negative integer. Not '{decimals}\")\n",
    "    \n",
    "    if miles not in [',', '_', None]:\n",
    "        raise ValueError(f\"Miles must be either ',', '_', or None. Not '{miles}\")\n",
    "    \n",
    "    try:\n",
    "        num = float(value)                                  # Convert to float if possible\n",
    "        if num % 1 == 0:                                    # it its a total integer number\n",
    "            decimals = 0\n",
    "        if miles:\n",
    "            return f\"{num:>{width}{miles}.{decimals}f}\"     # Ancho fijo, x decimales, alineado a la derecha\n",
    "        else:\n",
    "            return f\"{num:>{width}.{decimals}f}\"\n",
    "        \n",
    "    except (ValueError, TypeError):\n",
    "        return str(value).rjust(width)                      # Alinea también strings, para mantener la grilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe510044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_series(\n",
    "    data: Union[pd.Series, np.ndarray, dict, list, set, pd.DataFrame],\n",
    "    index: Optional[pd.Index] = None,\n",
    "    name: Optional[str] = None\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Converts input data into a pandas Series, optionally returning value counts.\n",
    "\n",
    "    This function accepts various data types and converts them into a pandas Series.\n",
    "    If `count=True`, it returns the frequency count of the values in the resulting Series.\n",
    "\n",
    "    Parameters:\n",
    "        data (Union[pd.Series, np.ndarray, dict, list, set, pd.DataFrame]):\n",
    "            The input data to convert. Supported types include:\n",
    "            - pd.Series: returned as-is or counted if `count=True`.\n",
    "            - np.ndarray: flattened and converted to a Series.\n",
    "            - dict: keys become the index, values are used for data.\n",
    "            - list or set: converted directly to a Series.\n",
    "            - pd.DataFrame:\n",
    "                - 1 column: converted directly to a Series.\n",
    "                - 2 columns: first column becomes the index, second becomes the values.\n",
    "\n",
    "        count (bool or int, optional): Whether to return value counts instead of raw data.\n",
    "            If True or 1, returns frequencies of each value. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A pandas Series representing the input data. If `count=True`, returns\n",
    "            the value counts of the data.\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If `data` is not one of the supported types.\n",
    "        ValueError: If `count` is not a boolean or integer 0/1.\n",
    "        ValueError: If DataFrame has more than 2 columns.\n",
    "\n",
    "    Examples:\n",
    "        >>> import pandas as pd\n",
    "        >>> to_serie_with_count([1, 2, 2, 3])\n",
    "        0    1\n",
    "        1    2\n",
    "        2    2\n",
    "        3    3\n",
    "        dtype: int64\n",
    "\n",
    "        >>> to_serie_with_count([1, 2, 2, 3], count=True)\n",
    "        2    2\n",
    "        1    1\n",
    "        3    1\n",
    "        dtype: int64\n",
    "\n",
    "        >>> df = pd.DataFrame({'Category': ['A', 'B', 'A'], 'Value': [10, 20, 30]})\n",
    "        >>> to_serie_with_count(df)\n",
    "        Category\n",
    "        A    10\n",
    "        B    20\n",
    "        A    30\n",
    "        Name: Value, dtype: int64\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate parameters - FUTURE\n",
    "    \n",
    "    if isinstance(data, pd.Series):                 # If data is already a series no conversion needed\n",
    "        sr = data                                  \n",
    "    elif isinstance(data, np.ndarray):              # If data is a NumPy array   \n",
    "        sr = pd.sr(data.flatten())\n",
    "    elif isinstance(data, (dict, list)):\n",
    "        sr = pd.sr(data)\n",
    "    elif isinstance(data, (set)):\n",
    "        sr = pd.sr(tuple(data))\n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "        if data.shape[1] == 1:                      # Also len(data.columns == 1)\n",
    "            sr = data.iloc[:, 0]\n",
    "        elif data.shape[1] == 2:                    # Index: first col, Data: 2nd Col\n",
    "            sr = data.set_index(data.columns[0])[data.columns[1]]\n",
    "        else:\n",
    "            raise ValueError(\"DataFrame must have 1 oer 2 columns. Categories and values for 2 columns cases.\")\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported data type: {type(data)}. \"\n",
    "                    \"Supported types: pd.sr, np.ndarray, pd.DataFrame, dict, list, set, and pd.DataFrame\")\n",
    "\n",
    "    if name:\n",
    "        sr.name = name\n",
    "\n",
    "    if index:\n",
    "        sr.index = index\n",
    "\n",
    "    return sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7068fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fdt(\n",
    "        data: Union[pd.Series, np.ndarray, dict, list, set, pd.DataFrame],\n",
    "        value_counts: Optional[bool] = False,\n",
    "        dropna: Optional[bool] = True,\n",
    "        na_position: Optional[str] = 'last',\n",
    "        include_pcts: Optional[bool] = True,\n",
    "        include_plain_relatives: Optional[bool] = True,\n",
    "        fmt_values: Optional[bool] = False,\n",
    "        order: Optional[str] = 'desc',\n",
    "        na_aside: Optional[bool] = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a Frequency Distribution Table (FDT) with absolute, relative, and cumulative frequencies.\n",
    "\n",
    "    This function converts various input data types into a structured DataFrame containing:\n",
    "    - Absolute frequencies\n",
    "    - Cumulative frequencies\n",
    "    - Relative frequencies (proportions and percentages)\n",
    "    - Cumulative relative frequencies (percentages)\n",
    "\n",
    "    Parameters:\n",
    "        data (Union[pd.Series, np.ndarray, dict, list, pd.DataFrame]): Input data.\n",
    "            If DataFrame, it will be converted to a Series using `to_series`.\n",
    "        value_counts (bool, optional): Whether to count occurrences if input is raw data.\n",
    "            Assumes data is not pre-counted. Default is False.\n",
    "        dropna (bool, optional): Whether to exclude NaN values when counting frequencies.\n",
    "            Default is True.\n",
    "        na_position (str, optional): Position of NaN values in the output:\n",
    "            - 'first': Place NaN at the top.\n",
    "            - 'last': Place NaN at the bottom (default).\n",
    "            - 'value': Keep NaN in its natural order.\n",
    "            Default is 'last'.\n",
    "        include_pcts (bool, optional): Whether to include percentage columns.\n",
    "            If False, only absolute and cumulative frequencies are returned.\n",
    "            Default is True.\n",
    "        include_plain_relatives (bool, optional): Whether to return relative and cumulative relative values.\n",
    "            If False, only frequency and percentage columns are included.\n",
    "            Default is True.\n",
    "        fmt_values (bool, optional): Whether to format numeric values using `_fmt_value_for_pd`.\n",
    "            Useful for improving readability in reports. Default is False.\n",
    "        order (str, optional): Sort order for the output:\n",
    "            - 'asc': Sort values ascending.\n",
    "            - 'desc': Sort values descending (default).\n",
    "            - 'ix_asc': Sort by index ascending.\n",
    "            - 'ix_desc': Sort by index descending.\n",
    "            - None: No sorting.\n",
    "            Default is 'desc'.\n",
    "        na_aside (bool, optional): Whether to separate NaN values from calculations but keep them in the output.\n",
    "            If True, NaNs are added at the end and not included in cumulative or relative calculations.\n",
    "            Default is True.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the frequency distribution table with the following columns\n",
    "        (depending on parameters):\n",
    "            - Frequency\n",
    "            - Cumulative Frequency\n",
    "            - Relative Frequency\n",
    "            - Cumulative Relative Freq.\n",
    "            - Relative Freq. [%]\n",
    "            - Cumulative Freq. [%]\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `sort` or `na_position` receive invalid values.\n",
    "\n",
    "    Notes:\n",
    "        - This function uses `to_series` to convert input data into a pandas Series.\n",
    "        - If `na_aside=True` and NaNs are present, they are placed separately and not included in relative calculations.\n",
    "        - Useful for exploratory data analysis and generating clean statistical summaries.\n",
    "\n",
    "    Example:\n",
    "        >>> import pandas as pd\n",
    "        >>> data = pd.Series(['A', 'B', 'A', 'C', 'B', 'B', None])\n",
    "        >>> fdt = get_fdt(data, sort='desc', fmt_values=True)\n",
    "        >>> print(fdt)\n",
    "              Frequency  Cumulative Frequency  Relative Freq. [%]  Cumulative Freq. [%]\n",
    "        B           3                   3                42.86                  42.86\n",
    "        A           2                   5                28.57                  71.43\n",
    "        C           1                   6                14.29                  85.71\n",
    "        Nulls       1                   7                14.29                 100.00\n",
    "    \"\"\"\n",
    "    columns = [\n",
    "        'Frequency',\n",
    "        'Cumulative Frequency',\n",
    "        'Relative Frequency',\n",
    "        'Cumulative Relative Freq.',\n",
    "        'Relative Freq. [%]',\n",
    "        'Cumulative Freq. [%]'\n",
    "    ]\n",
    "    # def _calculate_fdt_relatives(series):     # Revisar, no me gusta el flujo actual\n",
    "    \n",
    "    sr = to_series(data)\n",
    "    \n",
    "    if dropna:\n",
    "        sr = sr.dropna()\n",
    "\n",
    "    if value_counts:\n",
    "        sr = sr.value_counts(dropna=dropna, sort=False)\n",
    "\n",
    "    # Order de original Series to obtain the fdt in the same order as the original data\n",
    "    match order:\n",
    "        case 'asc':\n",
    "            sr = sr.sort_values()\n",
    "        case 'desc':\n",
    "            sr = sr.sort_values(ascending=False)\n",
    "        case 'ix_asc':\n",
    "            sr = sr.sort_index()\n",
    "        case 'ix_desc':\n",
    "            sr = sr.sort_index(ascending=False)\n",
    "        case None:\n",
    "            pass\n",
    "        case _:\n",
    "            raise ValueError(f\"Valid values for order: 'asc', 'desc', 'ix_asc', 'ix_desc', or None. Got '{order}'\")\n",
    "\n",
    "    # Handle NaN values \n",
    "    try:                            # To manage when there aren't NaNs\n",
    "        nan_value = sr[np.nan]\n",
    "        sr_without_nan = sr.drop(np.nan)\n",
    "    except:\n",
    "        nan_value = 0\n",
    "        sr_without_nan = sr.copy()  # If no NaNs, we keep the original series without changes\n",
    "    finally:\n",
    "        # Column that will then be concatenated to the end of the DF if the na_aside option is true\n",
    "        nan_row_df = pd.DataFrame(data = [nan_value], columns=[columns[0]], index=['Nulls'])      # Only 'Frequency' column.\n",
    "\n",
    "    match na_position:              # 1. locate the NaNs values\n",
    "        case 'first':\n",
    "            sr = pd.concat([pd.Series({np.nan: nan_value}), sr_without_nan])\n",
    "        case 'last':\n",
    "            sr = pd.concat([sr_without_nan, pd.Series({np.nan: nan_value})])\n",
    "        case 'value' | None:\n",
    "            pass\n",
    "        case _:\n",
    "            raise ValueError(f\"Valid values for na_position: 'first', 'last', 'value' or None. Got '{na_position}'\")\n",
    "        \n",
    "    if na_aside and nan_value:      # 2. define if NaNs count for relative and cumulative values.\n",
    "        sr = sr_without_nan         # series without nulls on which the relative values will be calculated\n",
    "\n",
    "    else:                           # if NaNs: 1. na_position, 2 na_aside\n",
    "        match na_position:          # 1. locate the NaNs values\n",
    "            case 'first':\n",
    "                sr = pd.concat([pd.Series({np.nan: nan_value}), sr_without_nan])\n",
    "            case 'last':\n",
    "                sr = pd.concat([sr_without_nan, pd.Series({np.nan: nan_value})])\n",
    "            case 'value' | None:\n",
    "                pass\n",
    "            case _:\n",
    "                raise ValueError(f\"Valid values for na_position: 'first', 'last', 'value' or None. Got '{na_position}'\")\n",
    "        \n",
    "        if na_aside:                # 2. define if NaNs count for relative and cumulative values.\n",
    "            sr = sr_without_nan     # series without nulls on which the relative values will be calculated\n",
    "            # Column that will then be concatenated to the end of the DF if the na_aside option is true\n",
    "            nan_row_df = pd.DataFrame(data = [nan_value], columns=[columns[0]], index=['Nulls'])      # Only 'Frequency' column.\n",
    "\n",
    "    # Central rutine: create the fdt, including relative and cumulative columns.\n",
    "    fdt = pd.DataFrame(sr)\n",
    "    fdt.columns = [columns[0]]\n",
    "    fdt[columns[1]] = fdt['Frequency'].cumsum()\n",
    "    fdt[columns[2]] = fdt['Frequency'] / fdt['Frequency'].sum()\n",
    "    fdt[columns[3]] = fdt['Relative Frequency'].cumsum()\n",
    "    fdt[columns[4]] = fdt['Relative Frequency'] * 100\n",
    "    fdt[columns[5]] = fdt['Cumulative Relative Freq.'] * 100\n",
    "\n",
    "    if na_aside and not dropna:             # We add nan_columns at the end\n",
    "        fdt = pd.concat([fdt, nan_row_df])\n",
    "\n",
    "    if not include_pcts:                    # Don't return percentage columns\n",
    "        fdt = fdt[columns[0:4]]\n",
    "    \n",
    "    if not include_plain_relatives:         # Don't return relative and plain cumulative\n",
    "        fdt = fdt[[columns[0], columns[4], columns[5]]]\n",
    "\n",
    "    if fmt_values:\n",
    "        fdt = fdt.map(_fmt_value_for_pd)\n",
    "        \n",
    "    return fdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eca3f17a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State\n",
       "California    119\n",
       "Virginia        4\n",
       "Arizona        11\n",
       "Oregon         11\n",
       "Nevada         17\n",
       "Colorado       11\n",
       "Utah            6\n",
       "Kansas          1\n",
       "Wyoming         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc = df['State'].value_counts(sort=False, dropna=True)\n",
    "vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66cfb53b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "nan",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jm-mtm\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: nan",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m vc.isna().sum()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mvc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnan\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jm-mtm\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\series.py:1130\u001b[39m, in \u001b[36mSeries.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[key]\n\u001b[32m   1129\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[32m   1133\u001b[39m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[32m   1134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jm-mtm\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\series.py:1246\u001b[39m, in \u001b[36mSeries._get_value\u001b[39m\u001b[34m(self, label, takeable)\u001b[39m\n\u001b[32m   1243\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[label]\n\u001b[32m   1245\u001b[39m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1246\u001b[39m loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[loc]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jm-mtm\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: nan"
     ]
    }
   ],
   "source": [
    "vc.isna().sum()\n",
    "vc[np.nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12c7d552",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'nan_row_df' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m fdt_vc1 = \u001b[43mget_fdt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_counts\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_position\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlast\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_pcts\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_plain_relatives\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt_values\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdesc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_aside\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# fdt_vc1 = get_fdt(vc, na_aside=False)\u001b[39;00m\n\u001b[32m      3\u001b[39m fdt_vc1\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 147\u001b[39m, in \u001b[36mget_fdt\u001b[39m\u001b[34m(data, value_counts, dropna, na_position, include_pcts, include_plain_relatives, fmt_values, order, na_aside)\u001b[39m\n\u001b[32m    144\u001b[39m fdt[columns[\u001b[32m5\u001b[39m]] = fdt[\u001b[33m'\u001b[39m\u001b[33mCumulative Relative Freq.\u001b[39m\u001b[33m'\u001b[39m] * \u001b[32m100\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_aside \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dropna:             \u001b[38;5;66;03m# We add nan_columns at the end\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     fdt = pd.concat([fdt, \u001b[43mnan_row_df\u001b[49m])\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m include_pcts:                    \u001b[38;5;66;03m# Don't return percentage columns\u001b[39;00m\n\u001b[32m    150\u001b[39m     fdt = fdt[columns[\u001b[32m0\u001b[39m:\u001b[32m4\u001b[39m]]\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'nan_row_df' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "fdt_vc1 = get_fdt(vc, value_counts=True, dropna=False, na_position='last', include_pcts=True, include_plain_relatives=True, fmt_values=True, order='desc', na_aside=True)\n",
    "# fdt_vc1 = get_fdt(vc, na_aside=False)\n",
    "fdt_vc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df9dd21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Cumulative Frequency</th>\n",
       "      <th>Relative Frequency</th>\n",
       "      <th>Cumulative Relative Freq.</th>\n",
       "      <th>Relative Freq. [%]</th>\n",
       "      <th>Cumulative Freq. [%]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.446</td>\n",
       "      <td>44.569</td>\n",
       "      <td>44.569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>86</td>\n",
       "      <td>205</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.768</td>\n",
       "      <td>32.210</td>\n",
       "      <td>76.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nevada</th>\n",
       "      <td>17</td>\n",
       "      <td>222</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.831</td>\n",
       "      <td>6.367</td>\n",
       "      <td>83.146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colorado</th>\n",
       "      <td>11</td>\n",
       "      <td>233</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.873</td>\n",
       "      <td>4.120</td>\n",
       "      <td>87.266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oregon</th>\n",
       "      <td>11</td>\n",
       "      <td>244</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.914</td>\n",
       "      <td>4.120</td>\n",
       "      <td>91.386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arizona</th>\n",
       "      <td>11</td>\n",
       "      <td>255</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.955</td>\n",
       "      <td>4.120</td>\n",
       "      <td>95.506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Utah</th>\n",
       "      <td>6</td>\n",
       "      <td>261</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.978</td>\n",
       "      <td>2.247</td>\n",
       "      <td>97.753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Virginia</th>\n",
       "      <td>4</td>\n",
       "      <td>265</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.993</td>\n",
       "      <td>1.498</td>\n",
       "      <td>99.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kansas</th>\n",
       "      <td>1</td>\n",
       "      <td>266</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.375</td>\n",
       "      <td>99.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wyoming</th>\n",
       "      <td>1</td>\n",
       "      <td>267</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1</td>\n",
       "      <td>0.375</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Frequency Cumulative Frequency Relative Frequency  \\\n",
       "State                                                          \n",
       "California       119                  119              0.446   \n",
       "NaN               86                  205              0.322   \n",
       "Nevada            17                  222              0.064   \n",
       "Colorado          11                  233              0.041   \n",
       "Oregon            11                  244              0.041   \n",
       "Arizona           11                  255              0.041   \n",
       "Utah               6                  261              0.022   \n",
       "Virginia           4                  265              0.015   \n",
       "Kansas             1                  266              0.004   \n",
       "Wyoming            1                  267              0.004   \n",
       "\n",
       "           Cumulative Relative Freq. Relative Freq. [%] Cumulative Freq. [%]  \n",
       "State                                                                         \n",
       "California                     0.446             44.569               44.569  \n",
       "NaN                            0.768             32.210               76.779  \n",
       "Nevada                         0.831              6.367               83.146  \n",
       "Colorado                       0.873              4.120               87.266  \n",
       "Oregon                         0.914              4.120               91.386  \n",
       "Arizona                        0.955              4.120               95.506  \n",
       "Utah                           0.978              2.247               97.753  \n",
       "Virginia                       0.993              1.498               99.251  \n",
       "Kansas                         0.996              0.375               99.625  \n",
       "Wyoming                            1              0.375                  100  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fdt_s1 = get_fdt(df['Country'], value_counts=True, sort='asc', dropna=False, na_position='value', fmt_values=True, na_aside=False)\n",
    "# fdt_s1\n",
    "fdt_s2 = get_fdt(df['State'], value_counts=True, dropna=False, na_aside=False, na_position='value', fmt_values=True)\n",
    "fdt_s2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f238ec73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['119 (65.7 %)', '17 (9.4 %)', '11 (6.1 %)', '11 (6.1 %)', '11 (6.1 %)', '6 (3.3 %)', '4 (2.2 %)', '1 (0.6 %)', '1 (0.6 %)']\n",
      "cumulative_pcts.iloc[0] = 65.74585635359117\n",
      "cumulative_pcts.iloc[1] = 75.13812154696133\n",
      "cumulative_pcts.iloc[2] = 81.21546961325966\n",
      "cumulative_pcts.iloc[3] = 87.292817679558\n",
      "cumulative_pcts.iloc[4] = 93.37016574585634\n",
      "cumulative_pcts.iloc[5] = 96.68508287292816\n",
      "cumulative_pcts.iloc[6] = 98.89502762430938\n",
      "cumulative_pcts.iloc[7] = 99.44751381215468\n",
      "cumulative_pcts.iloc[8] = 99.99999999999997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.float64(81.21546961325966)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Cumulative Frequency</th>\n",
       "      <th>Relative Frequency</th>\n",
       "      <th>Cumulative Relative Freq.</th>\n",
       "      <th>Relative Freq. [%]</th>\n",
       "      <th>Cumulative Freq. [%]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "      <td>0.657459</td>\n",
       "      <td>0.657459</td>\n",
       "      <td>65.745856</td>\n",
       "      <td>65.745856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nevada</th>\n",
       "      <td>17</td>\n",
       "      <td>136</td>\n",
       "      <td>0.093923</td>\n",
       "      <td>0.751381</td>\n",
       "      <td>9.392265</td>\n",
       "      <td>75.138122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arizona</th>\n",
       "      <td>11</td>\n",
       "      <td>147</td>\n",
       "      <td>0.060773</td>\n",
       "      <td>0.812155</td>\n",
       "      <td>6.077348</td>\n",
       "      <td>81.215470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oregon</th>\n",
       "      <td>11</td>\n",
       "      <td>158</td>\n",
       "      <td>0.060773</td>\n",
       "      <td>0.872928</td>\n",
       "      <td>6.077348</td>\n",
       "      <td>87.292818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colorado</th>\n",
       "      <td>11</td>\n",
       "      <td>169</td>\n",
       "      <td>0.060773</td>\n",
       "      <td>0.933702</td>\n",
       "      <td>6.077348</td>\n",
       "      <td>93.370166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Utah</th>\n",
       "      <td>6</td>\n",
       "      <td>175</td>\n",
       "      <td>0.033149</td>\n",
       "      <td>0.966851</td>\n",
       "      <td>3.314917</td>\n",
       "      <td>96.685083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Virginia</th>\n",
       "      <td>4</td>\n",
       "      <td>179</td>\n",
       "      <td>0.022099</td>\n",
       "      <td>0.988950</td>\n",
       "      <td>2.209945</td>\n",
       "      <td>98.895028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kansas</th>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>0.005525</td>\n",
       "      <td>0.994475</td>\n",
       "      <td>0.552486</td>\n",
       "      <td>99.447514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wyoming</th>\n",
       "      <td>1</td>\n",
       "      <td>181</td>\n",
       "      <td>0.005525</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.552486</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Frequency  Cumulative Frequency  Relative Frequency  \\\n",
       "State                                                             \n",
       "California        119                   119            0.657459   \n",
       "Nevada             17                   136            0.093923   \n",
       "Arizona            11                   147            0.060773   \n",
       "Oregon             11                   158            0.060773   \n",
       "Colorado           11                   169            0.060773   \n",
       "Utah                6                   175            0.033149   \n",
       "Virginia            4                   179            0.022099   \n",
       "Kansas              1                   180            0.005525   \n",
       "Wyoming             1                   181            0.005525   \n",
       "\n",
       "            Cumulative Relative Freq.  Relative Freq. [%]  \\\n",
       "State                                                       \n",
       "California                   0.657459           65.745856   \n",
       "Nevada                       0.751381            9.392265   \n",
       "Arizona                      0.812155            6.077348   \n",
       "Oregon                       0.872928            6.077348   \n",
       "Colorado                     0.933702            6.077348   \n",
       "Utah                         0.966851            3.314917   \n",
       "Virginia                     0.988950            2.209945   \n",
       "Kansas                       0.994475            0.552486   \n",
       "Wyoming                      1.000000            0.552486   \n",
       "\n",
       "            Cumulative Freq. [%]  \n",
       "State                             \n",
       "California             65.745856  \n",
       "Nevada                 75.138122  \n",
       "Arizona                81.215470  \n",
       "Oregon                 87.292818  \n",
       "Colorado               93.370166  \n",
       "Utah                   96.685083  \n",
       "Virginia               98.895028  \n",
       "Kansas                 99.447514  \n",
       "Wyoming               100.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdt_2 = get_fdt(df['State'], value_counts=True)    \n",
    "cumulative_pcts = fdt_2['Cumulative Freq. [%]']\n",
    "top_3_pct = cumulative_pcts.iloc[min(2, len(cumulative_pcts)-1)]\n",
    "\n",
    "labels = [f\"{fdt_2.iloc[ix, 0]} ({fdt_2.iloc[ix, -2]:.1f} %)\" for ix in range(fdt_2.shape[0])]\n",
    "print(labels)\n",
    "\n",
    "for iloc_ix in range(len(cumulative_pcts)):\n",
    "    print(f\"cumulative_pcts.iloc[{iloc_ix}] = {cumulative_pcts.iloc[iloc_ix]}\")\n",
    "\n",
    "display(len(cumulative_pcts))\n",
    "display(top_3_pct)\n",
    "fdt_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd06e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "822d6309",
   "metadata": {},
   "source": [
    "## Some Typing Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f116700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    1\n",
       "b    2\n",
       "c    3\n",
       "Name: example_series, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Union, Optional, Any, Literal, Sequence, TypeAlias\n",
    "import pandas as pd\n",
    "\n",
    "IndexElement: TypeAlias = Union[str, int, float, 'datetime.datetime']\n",
    "\n",
    "def to_series(\n",
    "    data: Union[pd.Series, np.ndarray, dict, list, set, pd.DataFrame],\n",
    "    index: Optional[Union[pd.Index, Sequence[IndexElement]]] = None,\n",
    "    name: Optional[str] = None\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Converts input data into a pandas Series, optionally returning value counts.\n",
    "    \"\"\"\n",
    "    return pd.Series(data, index=index, name=name)\n",
    "\n",
    "\n",
    "to_series([1, 2, 3], ['a', 'b', 'c'], name='example_series')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dc319fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some Typing Tests\n",
    "## Standard Libs\n",
    "from typing import Union, Optional, Any, Literal, Sequence, TypeAlias\n",
    "\n",
    "# Third-Party Libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter  # for pareto chart and ?\n",
    "import seaborn as sns\n",
    "## Claude - Qwen\n",
    "\n",
    "\n",
    "## Custom types for non-included typing annotations - Grok\n",
    "IndexElement: TypeAlias = Union[str, int, float, 'datetime.datetime', pd.Timestamp]\n",
    "\n",
    "\n",
    "def test_typing(\n",
    "        value: Union[int, float, str],\n",
    "        data: Optional[Union[pd.Index, Sequence[IndexElement]]] = None,\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Test function to demonstrate typing with Union.\n",
    "\n",
    "    Parameters:\n",
    "        value (Union[int, float, str]): The input value which can be an int, float, or str.\n",
    "\n",
    "    Returns:\n",
    "        str: A string representation of the input value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"Numeric value: {value}\")\n",
    "    elif isinstance(value, str):\n",
    "        print(f\"String value: {value}\")\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported type: {type(value)}\")\n",
    "    \n",
    "    if data is not None:\n",
    "        if isinstance(data, pd.Index):\n",
    "            print(f\"Data is a pandas Index with {len(data)} elements.\")\n",
    "        elif isinstance(data, (list, tuple, np.ndarray)):\n",
    "            print(f\"Data is a sequence with {len(data)} elements.\")\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported data type: {type(data)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d3da61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric value: 42\n",
      "Data is a pandas Index with 3 elements.\n"
     ]
    }
   ],
   "source": [
    "test_typing(42, data=pd.Index(['a', 'b', 'c']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e94091d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeAlias, Optional, Union, Sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "IndexElement: TypeAlias = Union[str, int, float, 'datetime.datetime', np.str_, np.int64, np.float64, np.datetime64]\n",
    "IndexLike: TypeAlias = Union[pd.Index, Sequence[IndexElement], NDArray[IndexElement]]\n",
    "\n",
    "def mi_funcion(data, index: Optional[IndexLike] = None) -> None:\n",
    "    if index is not None:\n",
    "        if isinstance(index, np.ndarray) and index.ndim != 1:\n",
    "            raise ValueError(\"El array de NumPy debe ser 1D para usarse como índice\")\n",
    "        index = pd.Index(index) if not isinstance(index, pd.Index) else index\n",
    "        print(\"Índice proporcionado:\", index)\n",
    "    else:\n",
    "        print(\"No se proporcionó índice, usando índice por defecto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e76514e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índice proporcionado: Index(['a', 'b', 'c'], dtype='object')\n",
      "Índice proporcionado: Index(['a', 'b', 'c'], dtype='object')\n",
      "No se proporcionó índice, usando índice por defecto\n"
     ]
    }
   ],
   "source": [
    "mi_funcion(data=pd.Series([1, 2, 3]), index=np.array(['a', 'b', 'c']))\n",
    "mi_funcion(data=pd.Series([1, 2, 3]), index=pd.Index(['a', 'b', 'c']))\n",
    "mi_funcion(data=pd.Series([1, 2, 3]))  # Sin índice"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
